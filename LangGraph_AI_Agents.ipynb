{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedanida/AI-Agents-using-Langraph/blob/main/LangGraph_AI_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyIMaCr-nz-b",
        "outputId": "97cb9b4b-5255-4ea8-b239-cc16ca20bb6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langgraph openai langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set your API keys here\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ABC\"  # Replace with your OpenAI key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"XYZ\"  # Replace with your LangSmith key\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph_Agent_Patterns\""
      ],
      "metadata": {
        "id": "0E__FGIkn6-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 1. Sequential Agent Workflow"
      ],
      "metadata": {
        "id": "jyGH4lxNoYbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
        "\n",
        "def step1(input):\n",
        "    return f\"Step 1 done with input: {input}\"\n",
        "\n",
        "def step2(input):\n",
        "    return f\"Step 2 processed: {input}\"\n",
        "\n",
        "sequential_chain = RunnableSequence(first=RunnableLambda(step1), last=RunnableLambda(step2))\n",
        "\n",
        "result = sequential_chain.invoke(\"Start\")\n",
        "print(\"Sequential Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "419DrICAn67t",
        "outputId": "e67b7a9a-6b2e-4e89-a060-bee1a43dbd1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential Result: Step 2 processed: Step 1 done with input: Start\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🤝 2. Multi-Agent Collaboration"
      ],
      "metadata": {
        "id": "A9orLge0ocSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def researcher(input):\n",
        "    return f\"Research on '{input}': Found 3 key points.\"\n",
        "\n",
        "def writer(input):\n",
        "    return f\"Writing blog using research: {input}\"\n",
        "\n",
        "multi_agent_chain = RunnableSequence(\n",
        "    first=RunnableLambda(researcher),\n",
        "    last=RunnableLambda(writer)\n",
        ")\n",
        "\n",
        "result = multi_agent_chain.invoke(\"AI Agents in LangGraph\")\n",
        "print(\"Multi-Agent Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTb54m6BoXIx",
        "outputId": "545dd113-715d-4908-ead6-77aeabab39d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Agent Result: Writing blog using research: Research on 'AI Agents in LangGraph': Found 3 key points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔀 3. Conditional Branching"
      ],
      "metadata": {
        "id": "2uADkfZXogjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
        "\n",
        "# Define the condition\n",
        "def is_urgent(x):\n",
        "    return \"urgent\" in x.lower()\n",
        "\n",
        "# Define handlers\n",
        "urgent_handler = RunnableLambda(lambda x: \"Handled with HIGH priority 🚨\")\n",
        "normal_handler = RunnableLambda(lambda x: \"Handled with normal workflow ✅\")\n",
        "\n",
        "# Correct usage: pass (condition, handler) as tuples, and default as a plain Runnable\n",
        "conditional_chain = RunnableBranch(\n",
        "    (is_urgent, urgent_handler),\n",
        "    normal_handler  # Default branch: NOT in a tuple\n",
        ")\n",
        "\n",
        "# Test it out\n",
        "print(conditional_chain.invoke(\"This is an urgent issue\"))\n",
        "print(conditional_chain.invoke(\"This is a regular update\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-sTSQCwoXGf",
        "outputId": "850c4d78-8779-4341-dc04-0d8182fba574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handled with HIGH priority 🚨\n",
            "Handled with normal workflow ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 4. Reactive Agent with Memory"
      ],
      "metadata": {
        "id": "SU6YbXi4okdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHMFHnMQpzfQ",
        "outputId": "ced3f2d7-5032-4d2d-ebf4-b34ebb08d030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Create memory and LLM\n",
        "memory = ConversationBufferMemory()\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Create conversation chain\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "# Example interaction\n",
        "print(conversation.predict(input=\"Hi, I'm working on LangGraph.\"))\n",
        "print(conversation.predict(input=\"What did I just say?\"))\n",
        "print(conversation.predict(input=\"Remind me what we discussed so far?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIA3nvvMoXDs",
        "outputId": "e68d9b55-ff04-48c0-a1ad-9fac37ba0302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-fa0638f91358>:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-16-fa0638f91358>:7: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0)\n",
            "<ipython-input-16-fa0638f91358>:10: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, I'm working on LangGraph.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hello! LangGraph is a fascinating project that focuses on language processing and graph theory. It involves analyzing and visualizing language data using graph structures to uncover patterns and relationships. How can I assist you with your work on LangGraph today?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, I'm working on LangGraph.\n",
            "AI: Hello! LangGraph is a fascinating project that focuses on language processing and graph theory. It involves analyzing and visualizing language data using graph structures to uncover patterns and relationships. How can I assist you with your work on LangGraph today?\n",
            "Human: What did I just say?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "You just mentioned that you are working on LangGraph, a project that combines language processing and graph theory to analyze and visualize language data.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, I'm working on LangGraph.\n",
            "AI: Hello! LangGraph is a fascinating project that focuses on language processing and graph theory. It involves analyzing and visualizing language data using graph structures to uncover patterns and relationships. How can I assist you with your work on LangGraph today?\n",
            "Human: What did I just say?\n",
            "AI: You just mentioned that you are working on LangGraph, a project that combines language processing and graph theory to analyze and visualize language data.\n",
            "Human: Remind me what we discussed so far?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "We have discussed LangGraph, a project that focuses on language processing and graph theory to analyze and visualize language data. You mentioned that you are working on this project and I offered to assist you with any questions or tasks related to LangGraph. Is there anything specific you would like to know or discuss further?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🛠️ 5. Tool-Using Agent"
      ],
      "metadata": {
        "id": "hldbNnE3omkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents.agent_types import AgentType\n",
        "import requests\n",
        "\n",
        "def get_weather(city):\n",
        "    return f\"Dummy weather data for {city}\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"WeatherTool\",\n",
        "        func=get_weather,\n",
        "        description=\"Get current weather info for a city\"\n",
        "    )\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent.run(\"What's the weather like in San Jose?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Rej-VJgqooNR",
        "outputId": "f831e7d6-f16e-436c-b9bb-9d3996e49f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use the WeatherTool to get the current weather in San Jose.\n",
            "Action: WeatherTool\n",
            "Action Input: San Jose\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDummy weather data for San Jose\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI should try again to get the actual weather data for San Jose.\n",
            "Action: WeatherTool\n",
            "Action Input: San Jose\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDummy weather data for San Jose\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI should try one more time to get the accurate weather data for San Jose.\n",
            "Action: WeatherTool\n",
            "Action Input: San Jose\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDummy weather data for San Jose\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI should try a different approach to get the actual weather data for San Jose.\n",
            "Action: WeatherTool\n",
            "Action Input: San Jose\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDummy weather data for San Jose\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI should check if there is an issue with the WeatherTool or if there is a problem with the data source for San Jose.\n",
            "Action: Check WeatherTool status\n",
            "Action Input: N/A\u001b[0m\n",
            "Observation: Check WeatherTool status is not a valid tool, try one of [WeatherTool].\n",
            "Thought:\u001b[32;1m\u001b[1;3mI should try using a different weather tool or source to get the accurate weather data for San Jose.\n",
            "Action: WeatherTool\n",
            "Action Input: San Jose\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDummy weather data for San Jose\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI should consider using a different method to obtain the weather information for San Jose.\n",
            "Final Answer: Unfortunately, I was unable to retrieve the accurate weather data for San Jose.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Unfortunately, I was unable to retrieve the accurate weather data for San Jose.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}