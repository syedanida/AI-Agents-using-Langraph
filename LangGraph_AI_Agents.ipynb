{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+k4FCVdsWSpqe5twQRNmu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedanida/AI-Agents-using-Langraph/blob/main/LangGraph_AI_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. The Expert Pattern"
      ],
      "metadata": {
        "id": "CSIh8q08HqIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BzUMROu3NsX-",
        "outputId": "9ad895d3-0a69-498d-d653-5bce1da19761"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.51)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.70.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.3.30)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain_openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n",
        "!pip install langgraph --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZk4P0wsN9-Y",
        "outputId": "3ff3caa3-5fb6-4db5-9b64-f5a521374032"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.30)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.30)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall \"langgraph[all]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "K5ijXXnGP4ME",
        "outputId": "1c013b51-f444-4262-9293-dcdc8b1dd245"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph[all]\n",
            "  Using cached langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\n",
            "\u001b[33mWARNING: langgraph 0.3.29 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting langchain-core<0.4,>=0.1 (from langgraph[all])\n",
            "  Using cached langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph[all])\n",
            "  Using cached langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph[all])\n",
            "  Using cached langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph[all])\n",
            "  Using cached langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph[all])\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached langsmith-0.3.30-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting typing-extensions>=4.7 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pydantic<3.0.0,>=2.5.2 (from langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph[all])\n",
            "  Using cached ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "Collecting httpx>=0.25.2 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting requests<3,>=2 (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph[all])\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph[all])\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Using cached langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "Using cached langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "Using cached langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Using cached langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Using cached langgraph-0.3.29-py3-none-any.whl (144 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached langsmith-0.3.30-py3-none-any.whl (358 kB)\n",
            "Using cached orjson-3.10.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "Using cached ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
            "Using cached pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Using cached zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: zstandard, xxhash, urllib3, typing-extensions, tenacity, sniffio, PyYAML, packaging, ormsgpack, orjson, jsonpointer, idna, h11, charset-normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, jsonpatch, httpcore, anyio, requests-toolbelt, pydantic, httpx, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: zstandard\n",
            "    Found existing installation: zstandard 0.23.0\n",
            "    Uninstalling zstandard-0.23.0:\n",
            "      Successfully uninstalled zstandard-0.23.0\n",
            "  Attempting uninstall: xxhash\n",
            "    Found existing installation: xxhash 3.5.0\n",
            "    Uninstalling xxhash-3.5.0:\n",
            "      Successfully uninstalled xxhash-3.5.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.1\n",
            "    Uninstalling sniffio-1.3.1:\n",
            "      Successfully uninstalled sniffio-1.3.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: ormsgpack\n",
            "    Found existing installation: ormsgpack 1.9.1\n",
            "    Uninstalling ormsgpack-1.9.1:\n",
            "      Successfully uninstalled ormsgpack-1.9.1\n",
            "  Attempting uninstall: orjson\n",
            "    Found existing installation: orjson 3.10.16\n",
            "    Uninstalling orjson-3.10.16:\n",
            "      Successfully uninstalled orjson-3.10.16\n",
            "  Attempting uninstall: jsonpointer\n",
            "    Found existing installation: jsonpointer 3.0.0\n",
            "    Uninstalling jsonpointer-3.0.0:\n",
            "      Successfully uninstalled jsonpointer-3.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: typing-inspection\n",
            "    Found existing installation: typing-inspection 0.4.0\n",
            "    Uninstalling typing-inspection-0.4.0:\n",
            "      Successfully uninstalled typing-inspection-0.4.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: jsonpatch\n",
            "    Found existing installation: jsonpatch 1.33\n",
            "    Uninstalling jsonpatch-1.33:\n",
            "      Successfully uninstalled jsonpatch-1.33\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.8\n",
            "    Uninstalling httpcore-1.0.8:\n",
            "      Successfully uninstalled httpcore-1.0.8\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.9.0\n",
            "    Uninstalling anyio-4.9.0:\n",
            "      Successfully uninstalled anyio-4.9.0\n",
            "  Attempting uninstall: requests-toolbelt\n",
            "    Found existing installation: requests-toolbelt 1.0.0\n",
            "    Uninstalling requests-toolbelt-1.0.0:\n",
            "      Successfully uninstalled requests-toolbelt-1.0.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.30\n",
            "    Uninstalling langsmith-0.3.30:\n",
            "      Successfully uninstalled langsmith-0.3.30\n",
            "  Attempting uninstall: langgraph-sdk\n",
            "    Found existing installation: langgraph-sdk 0.1.61\n",
            "    Uninstalling langgraph-sdk-0.1.61:\n",
            "      Successfully uninstalled langgraph-sdk-0.1.61\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.51\n",
            "    Uninstalling langchain-core-0.3.51:\n",
            "      Successfully uninstalled langchain-core-0.3.51\n",
            "  Attempting uninstall: langgraph-checkpoint\n",
            "    Found existing installation: langgraph-checkpoint 2.0.24\n",
            "    Uninstalling langgraph-checkpoint-2.0.24:\n",
            "      Successfully uninstalled langgraph-checkpoint-2.0.24\n",
            "  Attempting uninstall: langgraph-prebuilt\n",
            "    Found existing installation: langgraph-prebuilt 0.1.8\n",
            "    Uninstalling langgraph-prebuilt-0.1.8:\n",
            "      Successfully uninstalled langgraph-prebuilt-0.1.8\n",
            "  Attempting uninstall: langgraph\n",
            "    Found existing installation: langgraph 0.3.29\n",
            "    Uninstalling langgraph-0.3.29:\n",
            "      Successfully uninstalled langgraph-0.3.29\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 charset-normalizer-3.4.1 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.51 langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 langsmith-0.3.30 orjson-3.10.16 ormsgpack-1.9.1 packaging-24.2 pydantic-2.11.3 pydantic-core-2.33.1 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-extensions-4.13.2 typing-inspection-0.4.0 urllib3-2.4.0 xxhash-3.5.0 zstandard-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "anyio",
                  "certifi",
                  "charset_normalizer",
                  "h11",
                  "jsonpatch",
                  "jsonpointer",
                  "requests",
                  "requests_toolbelt",
                  "sniffio",
                  "tenacity",
                  "xxhash",
                  "yaml",
                  "zstandard"
                ]
              },
              "id": "79e58aca353d4c0eb353be03d025fe5c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9nklp5lPR3E",
        "outputId": "f13c70b4-b1ef-4bb3-cde8-736a6e2afb99"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langgraph\n",
            "Version: 0.3.29\n",
            "Summary: Building stateful, multi-actor applications with LLMs\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk, xxhash\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph-checkpoint\n",
        "from langgraph.checkpoint.memory import MemorySaver # Import MemorySaver from correct location"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hNS0xfQuQVsl",
        "outputId": "73435d55-3b8a-4bfb-832c-084df5984c5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph-checkpoint in /usr/local/lib/python3.11/dist-packages (2.0.24)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.2.38 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint) (0.3.51)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint) (1.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (0.3.30)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (2.11.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langgraph-checkpoint) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, TypedDict, Annotated, Sequence, Tuple\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.tools import BaseTool, tool\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "import langsmith\n",
        "\n",
        "# Import the checkpointing class - using the in-memory checkpoint\n",
        "# from langgraph.checkpoint import MemorySaver\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ABC\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"XYZ\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"expert-pattern-demo\"\n",
        "\n",
        "# Create a state for our graph\n",
        "class GraphState(TypedDict):\n",
        "    messages: Annotated[Sequence[Any], \"The messages in the conversation so far\"]\n",
        "    next_expert: str\n",
        "    research_results: Dict[str, Any]\n",
        "    final_answer: str\n",
        "\n",
        "# The different experts\n",
        "PROGRAMMING_EXPERT_SYSTEM = \"\"\"You are a programming expert specialized in Python coding.\n",
        "When approached with coding problems, you provide detailed solutions with explanations.\n",
        "Be specific, technical, and thorough in your answers.\n",
        "\"\"\"\n",
        "\n",
        "MATH_EXPERT_SYSTEM = \"\"\"You are a mathematics expert specialized in calculus, statistics, and linear algebra.\n",
        "When approached with math problems, you provide step-by-step solutions with clear explanations.\n",
        "Use mathematical notation when appropriate.\n",
        "\"\"\"\n",
        "\n",
        "PHYSICS_EXPERT_SYSTEM = \"\"\"You are a physics expert specialized in mechanics and electromagnetism.\n",
        "When approached with physics problems, you provide explanations based on fundamental physical laws.\n",
        "Use physics equations when appropriate and explain the physical intuition.\n",
        "\"\"\"\n",
        "\n",
        "COORDINATOR_SYSTEM = \"\"\"You are a coordinator that decides which expert should handle the incoming question.\n",
        "Available experts: 'programming', 'math', 'physics', or 'final'.\n",
        "Choose 'final' only when all needed experts have provided their input and a final answer can be generated.\n",
        "\"\"\"\n",
        "\n",
        "def build_expert(system_message: str) -> ChatOpenAI:\n",
        "    \"\"\"Build an expert with a specific system message.\"\"\"\n",
        "    return ChatOpenAI(\n",
        "        model=\"gpt-4\", # Changed to gpt-4\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "# Create the experts\n",
        "programming_expert = build_expert(PROGRAMMING_EXPERT_SYSTEM)\n",
        "math_expert = build_expert(MATH_EXPERT_SYSTEM)\n",
        "physics_expert = build_expert(PHYSICS_EXPERT_SYSTEM)\n",
        "\n",
        "# Create the coordinator\n",
        "coordinator_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=COORDINATOR_SYSTEM),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    SystemMessage(content=\"Based on the question and previous responses, which expert should handle this next? Reply with just 'programming', 'math', 'physics', or 'final'.\")\n",
        "])\n",
        "\n",
        "coordinator_chain = coordinator_prompt | ChatOpenAI(temperature=0) | (lambda x: x.content.strip().lower())\n",
        "\n",
        "# Define the nodes for our graph\n",
        "def route_to_expert(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"Route the query to the appropriate expert.\"\"\"\n",
        "    expert = state[\"next_expert\"]\n",
        "    if expert == \"final\":\n",
        "        return {\"next\": \"final_answer_generator\"}\n",
        "    elif expert == \"programming\":\n",
        "        return {\"next\": \"programming_expert\"}\n",
        "    elif expert == \"math\":\n",
        "        return {\"next\": \"math_expert\"}\n",
        "    elif expert == \"physics\":\n",
        "        return {\"next\": \"physics_expert\"}\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown expert: {expert}\")\n",
        "\n",
        "def call_programming_expert(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"Call the programming expert.\"\"\"\n",
        "    response = programming_expert.invoke(state[\"messages\"])\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"next\": \"coordinator\"\n",
        "    }\n",
        "\n",
        "def call_math_expert(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"Call the math expert.\"\"\"\n",
        "    # Embed the system message in the prompt\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=MATH_EXPERT_SYSTEM),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "    chain = prompt | math_expert\n",
        "    response = chain.invoke({\"messages\": state[\"messages\"]})\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"next\": \"coordinator\"\n",
        "    }\n",
        "\n",
        "def call_physics_expert(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"Call the physics expert.\"\"\"\n",
        "    response = physics_expert.invoke(state[\"messages\"])\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"next\": \"coordinator\"\n",
        "    }\n",
        "\n",
        "def call_coordinator(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"Call the coordinator to decide which expert to route to next.\"\"\"\n",
        "    next_expert = coordinator_chain.invoke({\"messages\": state[\"messages\"]})\n",
        "    return {\"next_expert\": next_expert}\n",
        "\n",
        "def generate_final_answer(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"Generate the final answer based on all expert inputs.\"\"\"\n",
        "    final_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=\"You are an assistant that synthesizes expert opinions to provide a comprehensive final answer.\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        SystemMessage(content=\"Based on all the expert input above, provide a comprehensive final answer.\")\n",
        "    ])\n",
        "\n",
        "    final_chain = final_prompt | ChatOpenAI(temperature=0)\n",
        "    response = final_chain.invoke({\"messages\": state[\"messages\"]})\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"final_answer\": response.content,\n",
        "        \"next\": END\n",
        "    }\n",
        "\n",
        "# Create the graph\n",
        "def build_expert_graph():\n",
        "    graph = StateGraph(GraphState)\n",
        "\n",
        "    # Add nodes\n",
        "    graph.add_node(\"coordinator\", call_coordinator)\n",
        "    graph.add_node(\"router\", route_to_expert)\n",
        "    graph.add_node(\"programming_expert\", call_programming_expert)\n",
        "    graph.add_node(\"math_expert\", call_math_expert)\n",
        "    graph.add_node(\"physics_expert\", call_physics_expert)\n",
        "    graph.add_node(\"final_answer_generator\", generate_final_answer)\n",
        "\n",
        "    # Add edges\n",
        "    graph.add_edge(\"coordinator\", \"router\")\n",
        "    graph.add_conditional_edges(\n",
        "        \"router\",\n",
        "        lambda x: x[\"next\"],\n",
        "        {\n",
        "            \"programming_expert\": \"programming_expert\",\n",
        "            \"math_expert\": \"math_expert\",\n",
        "            \"physics_expert\": \"physics_expert\",\n",
        "            \"final_answer_generator\": \"final_answer_generator\"\n",
        "        }\n",
        "    )\n",
        "    graph.add_edge(\"programming_expert\", \"coordinator\")\n",
        "    graph.add_edge(\"math_expert\", \"coordinator\")\n",
        "    graph.add_edge(\"physics_expert\", \"coordinator\")\n",
        "    graph.add_edge(\"final_answer_generator\", END)\n",
        "\n",
        "    # Set entry point\n",
        "    graph.set_entry_point(\"coordinator\")\n",
        "\n",
        "    # 🟢 Compile WITHOUT checkpointing\n",
        "    return graph.compile()\n",
        "\n",
        "# Create a runnable graph\n",
        "expert_graph = build_expert_graph()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize state\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"I need to calculate the volume of a sphere with radius r=5cm and then write a Python function to calculate this for any radius.\")],\n",
        "        \"next_expert\": \"coordinator\",\n",
        "        \"research_results\": {},\n",
        "        \"final_answer\": \"\"\n",
        "    }\n",
        "\n",
        "    # Run the graph with tracing\n",
        "    result = expert_graph.invoke(initial_state)\n",
        "\n",
        "    # Print final answer\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(result[\"final_answer\"])\n",
        "    print(\"\\nFull conversation:\")\n",
        "    for msg in result[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            print(f\"Human: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            print(f\"AI: {msg.content[:100]}...\")  # Print first 100 chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVbCBFlMHuEl",
        "outputId": "9e53da3e-4801-4cf2-d238-678763a6dfbf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Answer:\n",
            "To calculate the volume of a sphere with a radius of 5 cm, you can use the formula V = 4/3 * π * r³, where r is the radius. Substituting r = 5 cm into the formula gives V ≈ 523.6 cm³.\n",
            "\n",
            "To calculate the volume of a sphere for any given radius, you can use the Python function below:\n",
            "\n",
            "```python\n",
            "import math\n",
            "\n",
            "def volume_of_sphere(radius):\n",
            "    return (4/3) * math.pi * (radius**3)\n",
            "```\n",
            "\n",
            "This function takes the radius of the sphere as an argument and returns the volume of the sphere. The `math.pi` constant provides the value of π, and the `**` operator is used for exponentiation in Python.\n",
            "\n",
            "Full conversation:\n",
            "Human: I need to calculate the volume of a sphere with radius r=5cm and then write a Python function to calculate this for any radius.\n",
            "AI: The volume V of a sphere is given by the formula:\n",
            "\n",
            "V = 4/3 * π * r³\n",
            "\n",
            "where r is the radius of the sp...\n",
            "AI: To calculate the volume of a sphere with a radius of 5 cm, you can use the formula V = 4/3 * π * r³,...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. The Supervisor Pattern"
      ],
      "metadata": {
        "id": "ffkqOon2HuSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, TypedDict, Annotated, Sequence, Tuple, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, FunctionMessage\n",
        "from langchain_core.tools import BaseTool, tool\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "import langsmith\n",
        "#from langgraph.checkpoint import MemoryCheckpoint\n",
        "import json\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ABC\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"XYZ\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"supervisor-pattern-demo\"\n",
        "\n",
        "# Create a state for our graph\n",
        "class GraphState(TypedDict):\n",
        "    messages: Annotated[Sequence[Any], \"The messages in the conversation so far\"]\n",
        "    task_queue: Annotated[List[Dict], \"The queue of tasks to be executed\"]\n",
        "    completed_tasks: Annotated[List[Dict], \"The tasks that have been completed\"]\n",
        "    current_task: Annotated[Optional[Dict], \"The current task being executed\"]\n",
        "    final_report: Annotated[str, \"The final report summarizing all task results\"]\n",
        "\n",
        "# Create the supervisor system message\n",
        "SUPERVISOR_SYSTEM = \"\"\"You are a project supervisor responsible for breaking down complex projects into tasks,\n",
        "assigning them to workers, and synthesizing the results.\n",
        "\n",
        "Given a project request, you will:\n",
        "1. Break it down into smaller, manageable tasks\n",
        "2. Keep track of task progress\n",
        "3. Synthesize the completed task results into a final report\n",
        "\"\"\"\n",
        "\n",
        "# Create worker system message\n",
        "WORKER_SYSTEM = \"\"\"You are a competent worker responsible for completing assigned tasks.\n",
        "Complete the task to the best of your ability, providing detailed outputs.\n",
        "\"\"\"\n",
        "\n",
        "# Create our tools\n",
        "@tool\n",
        "def create_tasks(project_description: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Break down a project into individual tasks.\n",
        "    Args:\n",
        "        project_description: Description of the project to be broken down\n",
        "    Returns:\n",
        "        List of task dictionaries with 'id', 'description', and 'status'\n",
        "    \"\"\"\n",
        "    # This function simulates task breakdown - in production, the LLM would do this\n",
        "    return [\n",
        "        {\"id\": \"1\", \"description\": f\"Research phase for {project_description}\", \"status\": \"pending\"},\n",
        "        {\"id\": \"2\", \"description\": f\"Analysis phase for {project_description}\", \"status\": \"pending\"},\n",
        "        {\"id\": \"3\", \"description\": f\"Implementation phase for {project_description}\", \"status\": \"pending\"},\n",
        "        {\"id\": \"4\", \"description\": f\"Testing phase for {project_description}\", \"status\": \"pending\"},\n",
        "        {\"id\": \"5\", \"description\": f\"Documentation phase for {project_description}\", \"status\": \"pending\"}\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def assign_next_task() -> Dict:\n",
        "    \"\"\"\n",
        "    Assign the next task from the queue.\n",
        "    Returns:\n",
        "        The next task to be executed\n",
        "    \"\"\"\n",
        "    # This is just a placeholder - the actual implementation would happen in the state transition\n",
        "    return {\"id\": \"next_task_id\", \"description\": \"Next task description\", \"status\": \"in_progress\"}\n",
        "\n",
        "@tool\n",
        "def complete_task(task_id: str, result: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Mark a task as completed and store its result.\n",
        "    Args:\n",
        "        task_id: The ID of the task to mark as completed\n",
        "        result: The result of the completed task\n",
        "    Returns:\n",
        "        The updated task with status 'completed'\n",
        "    \"\"\"\n",
        "    # This is just a placeholder - the actual implementation would happen in the state transition\n",
        "    return {\"id\": task_id, \"description\": \"Task description\", \"status\": \"completed\", \"result\": result}\n",
        "\n",
        "@tool\n",
        "def generate_final_report(completed_tasks: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Generate a final report based on all completed tasks.\n",
        "    Args:\n",
        "        completed_tasks: List of all completed tasks with their results\n",
        "    Returns:\n",
        "        A comprehensive final report\n",
        "    \"\"\"\n",
        "    if not completed_tasks:\n",
        "        return \"No tasks were completed. No final report can be generated.\"\n",
        "\n",
        "    report_lines = [\"Final Report:\\n\"]\n",
        "    for task in completed_tasks:\n",
        "        line = f\"Task {task['id']} - {task['description']}\\nResult: {task.get('result', 'No result provided')}\\n\"\n",
        "        report_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(report_lines)\n",
        "\n",
        "\n",
        "# Build the supervisor and worker agents\n",
        "supervisor_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "worker_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
        "\n",
        "# Define node functions\n",
        "def initialize_project(state: GraphState) -> Dict:\n",
        "    \"\"\"Initialize the project by creating tasks.\"\"\"\n",
        "    # Extract project description from initial message\n",
        "    initial_message = state[\"messages\"][0]\n",
        "    project_description = initial_message.content\n",
        "\n",
        "    # Create tasks using the LLM\n",
        "    supervisor_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=SUPERVISOR_SYSTEM),\n",
        "        HumanMessage(content=f\"I need to break down this project into individual tasks: {project_description}\")\n",
        "    ])\n",
        "\n",
        "    supervisor_with_tools = supervisor_llm.bind(\n",
        "        functions=[convert_to_openai_function(create_tasks)]\n",
        "    )\n",
        "\n",
        "    # Pass a dictionary with input values to the prompt template\n",
        "    response = supervisor_with_tools.invoke(supervisor_prompt.format_prompt(input=project_description).to_messages())\n",
        "\n",
        "    # Parse the response to get the tasks\n",
        "    if hasattr(response, \"function_call\"):\n",
        "        function_name = response.function_call[\"name\"]\n",
        "        function_args = json.loads(response.function_call[\"arguments\"])\n",
        "\n",
        "        if function_name == \"create_tasks\":\n",
        "            tasks = create_tasks(function_args[\"project_description\"])\n",
        "        else:\n",
        "            tasks = []\n",
        "    else:\n",
        "        # Fallback if no function call was made\n",
        "        tasks = create_tasks(project_description)\n",
        "\n",
        "    # Update state\n",
        "    system_message = f\"Project initialized with {len(tasks)} tasks.\"\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=system_message)],\n",
        "        \"task_queue\": tasks,\n",
        "        \"completed_tasks\": [],\n",
        "        \"current_task\": None\n",
        "    }\n",
        "\n",
        "def decide_next_action(state: GraphState) -> Dict:\n",
        "    if state[\"task_queue\"]:\n",
        "        return {\"next\": \"assign_task\"}\n",
        "    else:\n",
        "        return {\"next\": \"generate_report\"}\n",
        "\n",
        "def assign_task(state: GraphState) -> Dict:\n",
        "    \"\"\"Assign the next task from the queue.\"\"\"\n",
        "    # Get the next task\n",
        "    next_task = state[\"task_queue\"][0]\n",
        "    remaining_tasks = state[\"task_queue\"][1:]\n",
        "\n",
        "    next_task[\"status\"] = \"in_progress\"\n",
        "\n",
        "    # Update state\n",
        "    system_message = f\"Assigning task {next_task['id']}: {next_task['description']}\"\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=system_message)],\n",
        "        \"task_queue\": remaining_tasks,\n",
        "        \"current_task\": next_task,\n",
        "        \"next\": \"execute_task\"\n",
        "    }\n",
        "\n",
        "def execute_task(state: GraphState) -> Dict:\n",
        "    task = state[\"current_task\"]\n",
        "    result = f\"Completed {task['description']}\"\n",
        "    task[\"result\"] = result\n",
        "    task[\"status\"] = \"completed\"\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=f\"Task {task['id']} completed: {result}\")],\n",
        "        \"completed_tasks\": state[\"completed_tasks\"] + [task],\n",
        "        \"current_task\": None\n",
        "    }\n",
        "\n",
        "def task_feedback(state: GraphState) -> Dict:\n",
        "    \"\"\"Provide feedback on the completed task.\"\"\"\n",
        "    latest_completed_task = state[\"completed_tasks\"][-1]\n",
        "\n",
        "    # Create supervisor feedback prompt\n",
        "    supervisor_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=SUPERVISOR_SYSTEM),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        HumanMessage(content=f\"Provide feedback on the completed task: {latest_completed_task['description']}\\n\\nResult: {latest_completed_task['result']}\")\n",
        "    ])\n",
        "\n",
        "    # Generate feedback\n",
        "    response = supervisor_llm.invoke(state[\"messages\"])  # Pass the list of messages directly\n",
        "\n",
        "    # Update state\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"next\": \"decision_point\"\n",
        "    }\n",
        "\n",
        "def generate_report(state: GraphState) -> Dict:\n",
        "    \"\"\"Generate a final report based on all completed tasks.\"\"\"\n",
        "    # Manually summarize the completed tasks\n",
        "    completed_summaries = [\n",
        "        f\"Task {task['id']} - {task['description']}: {task.get('result', 'No result')}\"\n",
        "        for task in state[\"completed_tasks\"]\n",
        "    ]\n",
        "    report = \"\\n\".join(completed_summaries)\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=\"Final Report:\\n\" + report)],\n",
        "        \"final_report\": report\n",
        "    }\n",
        "\n",
        "# Build the graph\n",
        "def build_supervisor_graph():\n",
        "    graph = StateGraph(GraphState)\n",
        "\n",
        "    # Add nodes\n",
        "    graph.add_node(\"initialize_project\", initialize_project)\n",
        "    graph.add_node(\"decision_point\", decide_next_action)\n",
        "    graph.add_node(\"assign_task\", assign_task)\n",
        "    graph.add_node(\"execute_task\", execute_task)\n",
        "    graph.add_node(\"task_feedback\", task_feedback)\n",
        "    graph.add_node(\"generate_report\", generate_report)\n",
        "\n",
        "    # Add edges\n",
        "    graph.add_edge(\"initialize_project\", \"decision_point\")\n",
        "    graph.add_conditional_edges(\n",
        "        \"decision_point\",\n",
        "        lambda x: x[\"next\"],\n",
        "        {\n",
        "            \"assign_task\": \"assign_task\",\n",
        "            \"generate_report\": \"generate_report\"\n",
        "        }\n",
        "    )\n",
        "    graph.add_edge(\"assign_task\", \"execute_task\")\n",
        "    graph.add_edge(\"execute_task\", \"task_feedback\")\n",
        "    graph.add_edge(\"task_feedback\", \"decision_point\")\n",
        "    graph.add_edge(\"generate_report\", END)\n",
        "\n",
        "    # Set entry point\n",
        "    graph.set_entry_point(\"initialize_project\")\n",
        "\n",
        "    # Compile without any checkpointer\n",
        "    return graph.compile()\n",
        "\n",
        "# Create a runnable graph\n",
        "supervisor_graph = build_supervisor_graph()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize state\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"Build a simple e-commerce website with product listings and a shopping cart\")],\n",
        "        \"task_queue\": [],\n",
        "        \"completed_tasks\": [],\n",
        "        \"current_task\": None,\n",
        "        \"final_report\": \"\"\n",
        "    }\n",
        "\n",
        "    # Run the graph with tracing\n",
        "    result = supervisor_graph.invoke(initial_state)\n",
        "\n",
        "    # Print final report\n",
        "    # Changed final_state to result\n",
        "    print(\"\\n\".join([\n",
        "       msg.content for msg in result[\"messages\"] if isinstance(msg, AIMessage)\n",
        "]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKkavnTcHw9U",
        "outputId": "a55c6053-9cde-4ad6-e7af-d72ae959591e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project initialized with 5 tasks.\n",
            "Assigning task 1: Research phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 1 completed: Completed Research phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "To build a simple e-commerce website with product listings and a shopping cart, you can follow these steps. This guide will help you create a basic e-commerce site using HTML, CSS, and JavaScript for the front end, and a simple backend using Node.js and Express. For simplicity, we'll use a JSON file to store product data and a session to manage the shopping cart.\n",
            "\n",
            "### Step 1: Set Up Your Project\n",
            "\n",
            "1. **Create a Project Directory:**\n",
            "   - Create a new directory for your project, e.g., `simple-ecommerce`.\n",
            "\n",
            "2. **Initialize Node.js:**\n",
            "   - Open a terminal in your project directory and run:\n",
            "     ```bash\n",
            "     npm init -y\n",
            "     ```\n",
            "   - This will create a `package.json` file.\n",
            "\n",
            "3. **Install Dependencies:**\n",
            "   - Install Express for the server:\n",
            "     ```bash\n",
            "     npm install express\n",
            "     ```\n",
            "   - Install additional packages for session management:\n",
            "     ```bash\n",
            "     npm install express-session\n",
            "     ```\n",
            "\n",
            "### Step 2: Create the Backend\n",
            "\n",
            "1. **Create the Server File:**\n",
            "   - Create a file named `server.js` in your project directory.\n",
            "\n",
            "2. **Set Up Express Server:**\n",
            "   - Add the following code to `server.js`:\n",
            "     ```javascript\n",
            "     const express = require('express');\n",
            "     const session = require('express-session');\n",
            "     const path = require('path');\n",
            "     const app = express();\n",
            "\n",
            "     // Middleware\n",
            "     app.use(express.json());\n",
            "     app.use(express.urlencoded({ extended: true }));\n",
            "     app.use(session({\n",
            "       secret: 'your-secret-key',\n",
            "       resave: false,\n",
            "       saveUninitialized: true\n",
            "     }));\n",
            "\n",
            "     // Serve static files\n",
            "     app.use(express.static(path.join(__dirname, 'public')));\n",
            "\n",
            "     // Sample product data\n",
            "     const products = [\n",
            "       { id: 1, name: 'Product 1', price: 10 },\n",
            "       { id: 2, name: 'Product 2', price: 20 },\n",
            "       { id: 3, name: 'Product 3', price: 30 }\n",
            "     ];\n",
            "\n",
            "     // API to get products\n",
            "     app.get('/api/products', (req, res) => {\n",
            "       res.json(products);\n",
            "     });\n",
            "\n",
            "     // API to add to cart\n",
            "     app.post('/api/cart', (req, res) => {\n",
            "       const productId = req.body.productId;\n",
            "       const product = products.find(p => p.id === productId);\n",
            "\n",
            "       if (!product) {\n",
            "         return res.status(404).send('Product not found');\n",
            "       }\n",
            "\n",
            "       if (!req.session.cart) {\n",
            "         req.session.cart = [];\n",
            "       }\n",
            "\n",
            "       req.session.cart.push(product);\n",
            "       res.send(req.session.cart);\n",
            "     });\n",
            "\n",
            "     // API to get cart\n",
            "     app.get('/api/cart', (req, res) => {\n",
            "       res.json(req.session.cart || []);\n",
            "     });\n",
            "\n",
            "     // Start server\n",
            "     const PORT = process.env.PORT || 3000;\n",
            "     app.listen(PORT, () => {\n",
            "       console.log(`Server is running on port ${PORT}`);\n",
            "     });\n",
            "     ```\n",
            "\n",
            "### Step 3: Create the Frontend\n",
            "\n",
            "1. **Create a Public Directory:**\n",
            "   - Inside your project directory, create a folder named `public`.\n",
            "\n",
            "2. **Create HTML File:**\n",
            "   - Inside the `public` directory, create an `index.html` file with the following content:\n",
            "     ```html\n",
            "     <!DOCTYPE html>\n",
            "     <html lang=\"en\">\n",
            "     <head>\n",
            "       <meta charset=\"UTF-8\">\n",
            "       <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "       <title>Simple E-commerce</title>\n",
            "       <link rel=\"stylesheet\" href=\"styles.css\">\n",
            "     </head>\n",
            "     <body>\n",
            "       <h1>Product Listings</h1>\n",
            "       <div id=\"products\"></div>\n",
            "       <h2>Shopping Cart</h2>\n",
            "       <div id=\"cart\"></div>\n",
            "       <script src=\"script.js\"></script>\n",
            "     </body>\n",
            "     </html>\n",
            "     ```\n",
            "\n",
            "3. **Create CSS File:**\n",
            "   - Inside the `public` directory, create a `styles.css` file for basic styling:\n",
            "     ```css\n",
            "     body {\n",
            "       font-family: Arial, sans-serif;\n",
            "       margin: 20px;\n",
            "     }\n",
            "     #products, #cart {\n",
            "       margin-top: 20px;\n",
            "     }\n",
            "     .product, .cart-item {\n",
            "       border: 1px solid #ccc;\n",
            "       padding: 10px;\n",
            "       margin-bottom: 10px;\n",
            "     }\n",
            "     ```\n",
            "\n",
            "4. **Create JavaScript File:**\n",
            "   - Inside the `public` directory, create a `script.js` file to handle frontend logic:\n",
            "     ```javascript\n",
            "     document.addEventListener('DOMContentLoaded', () => {\n",
            "       fetch('/api/products')\n",
            "         .then(response => response.json())\n",
            "         .then(products => {\n",
            "           const productsDiv = document.getElementById('products');\n",
            "           products.forEach(product => {\n",
            "             const productDiv = document.createElement('div');\n",
            "             productDiv.className = 'product';\n",
            "             productDiv.innerHTML = `\n",
            "               <h3>${product.name}</h3>\n",
            "               <p>Price: $${product.price}</p>\n",
            "               <button onclick=\"addToCart(${product.id})\">Add to Cart</button>\n",
            "             `;\n",
            "             productsDiv.appendChild(productDiv);\n",
            "           });\n",
            "         });\n",
            "\n",
            "       fetch('/api/cart')\n",
            "         .then(response => response.json())\n",
            "         .then(cart => {\n",
            "           updateCart(cart);\n",
            "         });\n",
            "     });\n",
            "\n",
            "     function addToCart(productId) {\n",
            "       fetch('/api/cart', {\n",
            "         method: 'POST',\n",
            "         headers: {\n",
            "           'Content-Type': 'application/json'\n",
            "         },\n",
            "         body: JSON.stringify({ productId })\n",
            "       })\n",
            "       .then(response => response.json())\n",
            "       .then(cart => {\n",
            "         updateCart(cart);\n",
            "       });\n",
            "     }\n",
            "\n",
            "     function updateCart(cart) {\n",
            "       const cartDiv = document.getElementById('cart');\n",
            "       cartDiv.innerHTML = '';\n",
            "       cart.forEach(item => {\n",
            "         const cartItemDiv = document.createElement('div');\n",
            "         cartItemDiv.className = 'cart-item';\n",
            "         cartItemDiv.innerHTML = `\n",
            "           <h4>${item.name}</h4>\n",
            "           <p>Price: $${item.price}</p>\n",
            "         `;\n",
            "         cartDiv.appendChild(cartItemDiv);\n",
            "       });\n",
            "     }\n",
            "     ```\n",
            "\n",
            "### Step 4: Run Your Application\n",
            "\n",
            "1. **Start the Server:**\n",
            "   - In your terminal, run:\n",
            "     ```bash\n",
            "     node server.js\n",
            "     ```\n",
            "   - This will start your server on port 3000.\n",
            "\n",
            "2. **Access Your Website:**\n",
            "   - Open a web browser and go to `http://localhost:3000`.\n",
            "   - You should see the product listings and be able to add items to the shopping cart.\n",
            "\n",
            "This setup provides a basic e-commerce website with product listings and a shopping cart. You can expand this by adding more features like user authentication, payment processing, and a database for storing product and user data.\n",
            "Assigning task 2: Analysis phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 2 completed: Completed Analysis phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "### Analysis Phase for Building a Simple E-commerce Website\n",
            "\n",
            "In this phase, we will analyze the requirements and outline the key components needed to build a simple e-commerce website with product listings and a shopping cart. This analysis will help in understanding the scope of the project and the necessary steps to implement it.\n",
            "\n",
            "#### Key Components\n",
            "\n",
            "1. **Product Listings:**\n",
            "   - Display a list of products with details such as name, price, and description.\n",
            "   - Each product should have an \"Add to Cart\" button.\n",
            "\n",
            "2. **Shopping Cart:**\n",
            "   - Allow users to add products to a shopping cart.\n",
            "   - Display the contents of the cart with product details and total price.\n",
            "   - Provide options to update quantities or remove items from the cart.\n",
            "\n",
            "3. **Backend Server:**\n",
            "   - Serve product data to the frontend.\n",
            "   - Handle requests to add items to the cart and manage cart sessions.\n",
            "\n",
            "4. **Frontend Interface:**\n",
            "   - User-friendly interface to display products and cart.\n",
            "   - Responsive design to ensure usability on different devices.\n",
            "\n",
            "5. **Data Management:**\n",
            "   - Use a JSON file or a simple database to store product information.\n",
            "   - Manage cart data using sessions.\n",
            "\n",
            "#### Functional Requirements\n",
            "\n",
            "- **Product Management:**\n",
            "  - Retrieve and display a list of products.\n",
            "  - Allow users to view product details.\n",
            "\n",
            "- **Cart Management:**\n",
            "  - Add products to the cart.\n",
            "  - View and update cart contents.\n",
            "  - Calculate total price of items in the cart.\n",
            "\n",
            "- **User Interaction:**\n",
            "  - Provide feedback on actions (e.g., item added to cart).\n",
            "  - Ensure smooth navigation between product listings and cart.\n",
            "\n",
            "#### Non-Functional Requirements\n",
            "\n",
            "- **Performance:**\n",
            "  - Fast loading times for product listings and cart updates.\n",
            "\n",
            "- **Scalability:**\n",
            "  - Ability to handle an increasing number of products and users.\n",
            "\n",
            "- **Security:**\n",
            "  - Secure handling of user sessions and data.\n",
            "\n",
            "- **Usability:**\n",
            "  - Intuitive and easy-to-use interface.\n",
            "\n",
            "#### Tools and Technologies\n",
            "\n",
            "- **Frontend:**\n",
            "  - HTML, CSS, JavaScript for building the user interface.\n",
            "\n",
            "- **Backend:**\n",
            "  - Node.js and Express for server-side logic.\n",
            "\n",
            "- **Data Storage:**\n",
            "  - JSON file for storing product data (or a simple database for scalability).\n",
            "\n",
            "- **Session Management:**\n",
            "  - Express-session for managing user sessions and cart data.\n",
            "\n",
            "#### Project Scope\n",
            "\n",
            "The project will focus on creating a basic e-commerce platform with essential features. Advanced features like user authentication, payment processing, and a comprehensive database will be considered for future enhancements.\n",
            "\n",
            "By completing this analysis, we have a clear understanding of the requirements and components needed to build the e-commerce website. The next steps will involve designing the architecture and implementing the solution.\n",
            "Assigning task 3: Implementation phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 3 completed: Completed Implementation phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "### Implementation Phase for Building a Simple E-commerce Website\n",
            "\n",
            "In this phase, we will implement the e-commerce website based on the analysis and design. The implementation will involve setting up the project structure, developing the backend and frontend components, and integrating them to create a functional e-commerce platform.\n",
            "\n",
            "#### Step-by-Step Implementation\n",
            "\n",
            "#### Step 1: Set Up Project Structure\n",
            "\n",
            "1. **Create Project Directory:**\n",
            "   - Create a directory named `simple-ecommerce`.\n",
            "\n",
            "2. **Initialize Node.js Project:**\n",
            "   - Navigate to the project directory and run:\n",
            "     ```bash\n",
            "     npm init -y\n",
            "     ```\n",
            "   - This will create a `package.json` file.\n",
            "\n",
            "3. **Install Required Packages:**\n",
            "   - Install Express and session management packages:\n",
            "     ```bash\n",
            "     npm install express express-session\n",
            "     ```\n",
            "\n",
            "#### Step 2: Develop Backend with Node.js and Express\n",
            "\n",
            "1. **Create Server File:**\n",
            "   - Create a file named `server.js` in the project directory.\n",
            "\n",
            "2. **Set Up Express Server:**\n",
            "   - Add the following code to `server.js`:\n",
            "     ```javascript\n",
            "     const express = require('express');\n",
            "     const session = require('express-session');\n",
            "     const path = require('path');\n",
            "     const app = express();\n",
            "\n",
            "     // Middleware\n",
            "     app.use(express.json());\n",
            "     app.use(express.urlencoded({ extended: true }));\n",
            "     app.use(session({\n",
            "       secret: 'your-secret-key',\n",
            "       resave: false,\n",
            "       saveUninitialized: true\n",
            "     }));\n",
            "\n",
            "     // Serve static files\n",
            "     app.use(express.static(path.join(__dirname, 'public')));\n",
            "\n",
            "     // Sample product data\n",
            "     const products = [\n",
            "       { id: 1, name: 'Product 1', price: 10 },\n",
            "       { id: 2, name: 'Product 2', price: 20 },\n",
            "       { id: 3, name: 'Product 3', price: 30 }\n",
            "     ];\n",
            "\n",
            "     // API to get products\n",
            "     app.get('/api/products', (req, res) => {\n",
            "       res.json(products);\n",
            "     });\n",
            "\n",
            "     // API to add to cart\n",
            "     app.post('/api/cart', (req, res) => {\n",
            "       const productId = req.body.productId;\n",
            "       const product = products.find(p => p.id === productId);\n",
            "\n",
            "       if (!product) {\n",
            "         return res.status(404).send('Product not found');\n",
            "       }\n",
            "\n",
            "       if (!req.session.cart) {\n",
            "         req.session.cart = [];\n",
            "       }\n",
            "\n",
            "       req.session.cart.push(product);\n",
            "       res.send(req.session.cart);\n",
            "     });\n",
            "\n",
            "     // API to get cart\n",
            "     app.get('/api/cart', (req, res) => {\n",
            "       res.json(req.session.cart || []);\n",
            "     });\n",
            "\n",
            "     // Start server\n",
            "     const PORT = process.env.PORT || 3000;\n",
            "     app.listen(PORT, () => {\n",
            "       console.log(`Server is running on port ${PORT}`);\n",
            "     });\n",
            "     ```\n",
            "\n",
            "#### Step 3: Develop Frontend with HTML, CSS, and JavaScript\n",
            "\n",
            "1. **Create Public Directory:**\n",
            "   - Inside the project directory, create a folder named `public`.\n",
            "\n",
            "2. **Create HTML File:**\n",
            "   - Inside the `public` directory, create an `index.html` file:\n",
            "     ```html\n",
            "     <!DOCTYPE html>\n",
            "     <html lang=\"en\">\n",
            "     <head>\n",
            "       <meta charset=\"UTF-8\">\n",
            "       <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "       <title>Simple E-commerce</title>\n",
            "       <link rel=\"stylesheet\" href=\"styles.css\">\n",
            "     </head>\n",
            "     <body>\n",
            "       <h1>Product Listings</h1>\n",
            "       <div id=\"products\"></div>\n",
            "       <h2>Shopping Cart</h2>\n",
            "       <div id=\"cart\"></div>\n",
            "       <script src=\"script.js\"></script>\n",
            "     </body>\n",
            "     </html>\n",
            "     ```\n",
            "\n",
            "3. **Create CSS File:**\n",
            "   - Inside the `public` directory, create a `styles.css` file:\n",
            "     ```css\n",
            "     body {\n",
            "       font-family: Arial, sans-serif;\n",
            "       margin: 20px;\n",
            "     }\n",
            "     #products, #cart {\n",
            "       margin-top: 20px;\n",
            "     }\n",
            "     .product, .cart-item {\n",
            "       border: 1px solid #ccc;\n",
            "       padding: 10px;\n",
            "       margin-bottom: 10px;\n",
            "     }\n",
            "     ```\n",
            "\n",
            "4. **Create JavaScript File:**\n",
            "   - Inside the `public` directory, create a `script.js` file:\n",
            "     ```javascript\n",
            "     document.addEventListener('DOMContentLoaded', () => {\n",
            "       fetch('/api/products')\n",
            "         .then(response => response.json())\n",
            "         .then(products => {\n",
            "           const productsDiv = document.getElementById('products');\n",
            "           products.forEach(product => {\n",
            "             const productDiv = document.createElement('div');\n",
            "             productDiv.className = 'product';\n",
            "             productDiv.innerHTML = `\n",
            "               <h3>${product.name}</h3>\n",
            "               <p>Price: $${product.price}</p>\n",
            "               <button onclick=\"addToCart(${product.id})\">Add to Cart</button>\n",
            "             `;\n",
            "             productsDiv.appendChild(productDiv);\n",
            "           });\n",
            "         });\n",
            "\n",
            "       fetch('/api/cart')\n",
            "         .then(response => response.json())\n",
            "         .then(cart => {\n",
            "           updateCart(cart);\n",
            "         });\n",
            "     });\n",
            "\n",
            "     function addToCart(productId) {\n",
            "       fetch('/api/cart', {\n",
            "         method: 'POST',\n",
            "         headers: {\n",
            "           'Content-Type': 'application/json'\n",
            "         },\n",
            "         body: JSON.stringify({ productId })\n",
            "       })\n",
            "       .then(response => response.json())\n",
            "       .then(cart => {\n",
            "         updateCart(cart);\n",
            "       });\n",
            "     }\n",
            "\n",
            "     function updateCart(cart) {\n",
            "       const cartDiv = document.getElementById('cart');\n",
            "       cartDiv.innerHTML = '';\n",
            "       cart.forEach(item => {\n",
            "         const cartItemDiv = document.createElement('div');\n",
            "         cartItemDiv.className = 'cart-item';\n",
            "         cartItemDiv.innerHTML = `\n",
            "           <h4>${item.name}</h4>\n",
            "           <p>Price: $${item.price}</p>\n",
            "         `;\n",
            "         cartDiv.appendChild(cartItemDiv);\n",
            "       });\n",
            "     }\n",
            "     ```\n",
            "\n",
            "#### Step 4: Run and Test the Application\n",
            "\n",
            "1. **Start the Server:**\n",
            "   - In the terminal, run:\n",
            "     ```bash\n",
            "     node server.js\n",
            "     ```\n",
            "   - This will start the server on port 3000.\n",
            "\n",
            "2. **Access the Website:**\n",
            "   - Open a web browser and navigate to `http://localhost:3000`.\n",
            "   - Verify that the product listings are displayed and that you can add items to the shopping cart.\n",
            "\n",
            "This implementation provides a basic e-commerce website with product listings and a shopping cart. You can further enhance the application by adding features such as user authentication, payment processing, and a database for storing product and user data.\n",
            "Assigning task 4: Testing phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 4 completed: Completed Testing phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "### Testing Phase for Building a Simple E-commerce Website\n",
            "\n",
            "In this phase, we will conduct testing to ensure that the e-commerce website functions correctly and meets the specified requirements. Testing will involve verifying the functionality of product listings, the shopping cart, and the overall user experience.\n",
            "\n",
            "#### Testing Strategy\n",
            "\n",
            "1. **Unit Testing:**\n",
            "   - Test individual components and functions to ensure they work as expected.\n",
            "   - Focus on backend API endpoints and frontend JavaScript functions.\n",
            "\n",
            "2. **Integration Testing:**\n",
            "   - Test the interaction between different components of the application.\n",
            "   - Ensure that the frontend and backend communicate correctly.\n",
            "\n",
            "3. **User Interface Testing:**\n",
            "   - Verify that the user interface is intuitive and responsive.\n",
            "   - Test the layout and design on different devices and screen sizes.\n",
            "\n",
            "4. **Functional Testing:**\n",
            "   - Test the core functionalities of the website, such as product listings and cart management.\n",
            "   - Ensure that all user actions produce the expected results.\n",
            "\n",
            "5. **Regression Testing:**\n",
            "   - Re-test the application after making changes to ensure that existing functionality is not broken.\n",
            "\n",
            "#### Test Cases\n",
            "\n",
            "1. **Product Listings:**\n",
            "   - Verify that all products are displayed with correct details (name, price).\n",
            "   - Ensure that the \"Add to Cart\" button is present for each product.\n",
            "\n",
            "2. **Add to Cart:**\n",
            "   - Test adding a product to the cart and verify that it appears in the cart.\n",
            "   - Check that the cart updates correctly when multiple products are added.\n",
            "\n",
            "3. **View Cart:**\n",
            "   - Verify that the cart displays all added products with correct details.\n",
            "   - Ensure that the total price is calculated correctly.\n",
            "\n",
            "4. **Remove from Cart:**\n",
            "   - Test removing a product from the cart and verify that it is removed.\n",
            "   - Check that the total price updates correctly after removal.\n",
            "\n",
            "5. **Session Management:**\n",
            "   - Verify that the cart persists across page reloads using sessions.\n",
            "   - Test that the cart is cleared when the session ends.\n",
            "\n",
            "6. **Responsive Design:**\n",
            "   - Test the website on different devices (desktop, tablet, mobile) to ensure responsiveness.\n",
            "   - Verify that the layout adjusts correctly to different screen sizes.\n",
            "\n",
            "#### Testing Tools\n",
            "\n",
            "- **Manual Testing:**\n",
            "  - Conduct manual testing for user interface and functional testing.\n",
            "  - Use different browsers and devices to test responsiveness.\n",
            "\n",
            "- **Automated Testing:**\n",
            "  - Use tools like Mocha and Chai for unit and integration testing of the backend.\n",
            "  - Consider using Selenium or Cypress for automated UI testing.\n",
            "\n",
            "#### Testing Results\n",
            "\n",
            "- **Pass/Fail Criteria:**\n",
            "  - A test case passes if the actual result matches the expected result.\n",
            "  - A test case fails if there is a discrepancy between the actual and expected results.\n",
            "\n",
            "- **Bug Reporting:**\n",
            "  - Document any issues or bugs found during testing.\n",
            "  - Provide detailed steps to reproduce the issue and expected vs. actual results.\n",
            "\n",
            "By completing the testing phase, we ensure that the e-commerce website is functional, user-friendly, and meets the specified requirements. Any identified issues should be addressed before the final deployment.\n",
            "Assigning task 5: Documentation phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 5 completed: Completed Documentation phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "### Documentation Phase for Building a Simple E-commerce Website\n",
            "\n",
            "In this phase, we will create comprehensive documentation for the e-commerce website. This documentation will serve as a guide for developers, users, and stakeholders, providing insights into the system's architecture, functionality, and usage.\n",
            "\n",
            "#### Project Overview\n",
            "\n",
            "The simple e-commerce website is designed to provide basic functionalities for browsing products and managing a shopping cart. It includes a frontend interface for users to interact with and a backend server to handle data processing and storage.\n",
            "\n",
            "#### System Architecture\n",
            "\n",
            "1. **Frontend:**\n",
            "   - Built using HTML, CSS, and JavaScript.\n",
            "   - Provides a user interface for displaying product listings and managing the shopping cart.\n",
            "\n",
            "2. **Backend:**\n",
            "   - Developed with Node.js and Express.\n",
            "   - Handles API requests for product data and cart management.\n",
            "   - Manages user sessions to persist cart data.\n",
            "\n",
            "3. **Data Storage:**\n",
            "   - Uses a JSON file to store product information.\n",
            "   - Session data is stored in memory for managing the shopping cart.\n",
            "\n",
            "#### Key Features\n",
            "\n",
            "1. **Product Listings:**\n",
            "   - Displays a list of products with details such as name and price.\n",
            "   - Allows users to add products to the shopping cart.\n",
            "\n",
            "2. **Shopping Cart:**\n",
            "   - Displays items added to the cart with their details and total price.\n",
            "   - Provides functionality to update quantities or remove items.\n",
            "\n",
            "3. **Responsive Design:**\n",
            "   - Ensures the website is accessible and usable on various devices and screen sizes.\n",
            "\n",
            "#### Installation and Setup\n",
            "\n",
            "1. **Prerequisites:**\n",
            "   - Node.js and npm installed on your system.\n",
            "\n",
            "2. **Installation Steps:**\n",
            "   - Clone the repository or download the project files.\n",
            "   - Navigate to the project directory in the terminal.\n",
            "   - Run `npm install` to install the required dependencies.\n",
            "\n",
            "3. **Running the Application:**\n",
            "   - Start the server by running `node server.js`.\n",
            "   - Open a web browser and go to `http://localhost:3000` to access the website.\n",
            "\n",
            "#### API Endpoints\n",
            "\n",
            "1. **GET /api/products:**\n",
            "   - Retrieves the list of available products.\n",
            "   - Response: JSON array of product objects.\n",
            "\n",
            "2. **POST /api/cart:**\n",
            "   - Adds a product to the shopping cart.\n",
            "   - Request Body: JSON object with `productId`.\n",
            "   - Response: JSON array of cart items.\n",
            "\n",
            "3. **GET /api/cart:**\n",
            "   - Retrieves the current contents of the shopping cart.\n",
            "   - Response: JSON array of cart items.\n",
            "\n",
            "#### Testing and Quality Assurance\n",
            "\n",
            "- **Testing Strategy:**\n",
            "  - Includes unit, integration, and user interface testing.\n",
            "  - Ensures that all functionalities work as expected and the user experience is smooth.\n",
            "\n",
            "- **Testing Tools:**\n",
            "  - Manual testing for UI and functional testing.\n",
            "  - Automated testing tools like Mocha and Chai for backend testing.\n",
            "\n",
            "#### Future Enhancements\n",
            "\n",
            "- **User Authentication:**\n",
            "  - Implement user login and registration features.\n",
            "\n",
            "- **Payment Processing:**\n",
            "  - Integrate payment gateways for handling transactions.\n",
            "\n",
            "- **Database Integration:**\n",
            "  - Use a database like MongoDB or MySQL for storing product and user data.\n",
            "\n",
            "#### Conclusion\n",
            "\n",
            "This documentation provides a comprehensive overview of the simple e-commerce website, covering its architecture, features, installation, and testing. It serves as a valuable resource for understanding and maintaining the system, as well as planning future enhancements.\n",
            "Final Report:\n",
            "Task 1 - Research phase for Build a simple e-commerce website with product listings and a shopping cart: Completed Research phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 2 - Analysis phase for Build a simple e-commerce website with product listings and a shopping cart: Completed Analysis phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 3 - Implementation phase for Build a simple e-commerce website with product listings and a shopping cart: Completed Implementation phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 4 - Testing phase for Build a simple e-commerce website with product listings and a shopping cart: Completed Testing phase for Build a simple e-commerce website with product listings and a shopping cart\n",
            "Task 5 - Documentation phase for Build a simple e-commerce website with product listings and a shopping cart: Completed Documentation phase for Build a simple e-commerce website with product listings and a shopping cart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. The Reflexion Pattern"
      ],
      "metadata": {
        "id": "ezvOAXPCHyq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, TypedDict, Annotated, Sequence, Tuple, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, FunctionMessage\n",
        "from langchain_core.tools import BaseTool, tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "import json\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ABC\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"XYZ\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"reflexion-pattern-demo\"\n",
        "\n",
        "# Create a state for our graph\n",
        "class GraphState(TypedDict):\n",
        "    messages: Annotated[Sequence[Any], \"The messages in the conversation so far\"]\n",
        "    reflections: Annotated[List[str], \"The agent's reflections on its previous responses\"]\n",
        "    response_draft: Annotated[str, \"The current draft response being considered\"]\n",
        "    final_response: Annotated[str, \"The final response after reflection\"]\n",
        "    reflection_count: Annotated[int, \"How many times the agent has reflected\"]\n",
        "    max_reflections: Annotated[int, \"Maximum number of reflections to perform\"]\n",
        "\n",
        "# Create system messages\n",
        "AGENT_SYSTEM = \"\"\"You are a thoughtful assistant that generates careful and helpful responses.\n",
        "You will be given a question to answer. You will:\n",
        "1. Generate an initial response draft\n",
        "2. Reflect on that draft to identify potential issues or improvements\n",
        "3. Revise your response based on your reflection\n",
        "4. Repeat this process until you have a high-quality response or reach the maximum number of reflections\n",
        "\"\"\"\n",
        "\n",
        "REFLECTION_SYSTEM = \"\"\"You are a reflective critic that helps identify weaknesses in responses.\n",
        "Look critically at the response and identify any issues such as:\n",
        "- Factual errors or inconsistencies\n",
        "- Incomplete or partial responses\n",
        "- Logical fallacies or weak reasoning\n",
        "- Unclear or ambiguous language\n",
        "- Missing important considerations or perspectives\n",
        "- Potential harmful or misleading content\n",
        "\n",
        "Be specific and constructive in your criticism.\n",
        "\"\"\"\n",
        "\n",
        "REVISION_SYSTEM = \"\"\"You are a thoughtful reviser that improves responses based on reflection.\n",
        "Take the original response and the reflection feedback, and create an improved response that:\n",
        "- Addresses all issues identified in the reflection\n",
        "- Maintains the helpful aspects of the original response\n",
        "- Is clear, accurate, comprehensive, and balanced\n",
        "\"\"\"\n",
        "\n",
        "# Define LLM instances\n",
        "response_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "reflection_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "revision_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
        "\n",
        "# Define node functions\n",
        "def generate_initial_response(state: GraphState) -> Dict:\n",
        "    \"\"\"Generate an initial response to the user's query.\"\"\"\n",
        "    # Extract query from messages\n",
        "    query = state[\"messages\"][0].content\n",
        "\n",
        "    # Create response prompt\n",
        "    response_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=AGENT_SYSTEM),\n",
        "        HumanMessage(content=f\"Generate an initial response to this query: {query}\")\n",
        "    ])\n",
        "\n",
        "    # Generate response\n",
        "    response = response_llm.invoke(response_prompt.format_prompt(input_variables={'query': query}).to_messages())\n",
        "\n",
        "    # Update state\n",
        "    return {\n",
        "        \"response_draft\": response.content,\n",
        "        \"reflections\": [],\n",
        "        \"reflection_count\": 0\n",
        "    }\n",
        "\n",
        "def reflect_on_response(state: GraphState) -> Dict:\n",
        "    \"\"\"Reflect on the current response draft.\"\"\"\n",
        "    # Get the current query and response draft\n",
        "    query = state[\"messages\"][0].content\n",
        "    current_draft = state[\"response_draft\"]\n",
        "\n",
        "    # Create reflection prompt\n",
        "    reflection_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=REFLECTION_SYSTEM),\n",
        "        HumanMessage(content=f\"Query: {query}\\n\\nResponse draft:\\n{current_draft}\\n\\nReflect on this response draft and identify any issues or areas for improvement:\")\n",
        "    ])\n",
        "\n",
        "    # Generate reflection\n",
        "    # Format the prompt and convert it to a list of BaseMessages\n",
        "    reflection = reflection_llm.invoke(reflection_prompt.format_prompt(input_variables={'query': query, 'current_draft': current_draft}).to_messages())\n",
        "\n",
        "    # Update state\n",
        "    return {\n",
        "        \"reflections\": state[\"reflections\"] + [reflection.content],\n",
        "        \"reflection_count\": state[\"reflection_count\"] + 1\n",
        "    }\n",
        "\n",
        "def revise_response(state: GraphState) -> Dict:\n",
        "    \"\"\"Revise the response based on the reflection.\"\"\"\n",
        "    # Get the current query, response draft, and latest reflection\n",
        "    query = state[\"messages\"][0].content\n",
        "    current_draft = state[\"response_draft\"]\n",
        "    latest_reflection = state[\"reflections\"][-1]\n",
        "\n",
        "    # Create revision prompt\n",
        "    revision_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=REVISION_SYSTEM),\n",
        "        HumanMessage(content=f\"Query: {query}\\n\\nOriginal response draft:\\n{current_draft}\\n\\nReflection:\\n{latest_reflection}\\n\\nPlease revise the response to address the issues identified in the reflection:\")\n",
        "    ])\n",
        "\n",
        "    # Generate revised response\n",
        "    # revision = revision_llm.invoke(revision_prompt) # This is line 116\n",
        "    revision = revision_llm.invoke(revision_prompt.format_prompt(input_variables={'query': query, 'current_draft': current_draft, 'latest_reflection': latest_reflection}).to_messages()) # Format the prompt and convert to BaseMessages\n",
        "\n",
        "    # Update state\n",
        "    return {\n",
        "        \"response_draft\": revision.content\n",
        "    }\n",
        "\n",
        "def decide_next_step(state: GraphState) -> Dict:\n",
        "    \"\"\"Decide whether to reflect more or finalize the response.\"\"\"\n",
        "    if state[\"reflection_count\"] < state[\"max_reflections\"]:\n",
        "        # Check if the latest reflection suggests more improvements are needed\n",
        "        latest_reflection = state[\"reflections\"][-1]\n",
        "\n",
        "        # Create decision prompt\n",
        "        decision_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=\"Determine if the response needs further improvement based on the reflection.\"),\n",
        "            HumanMessage(content=f\"Reflection: {latest_reflection}\\n\\nBased on this reflection, does the response need further improvement? Answer with 'yes' or 'no'.\")\n",
        "        ])\n",
        "\n",
        "        # Generate the formatted prompt and invoke the model\n",
        "        formatted_prompt = decision_prompt.format_prompt()  # Correct usage here\n",
        "        decision = response_llm.invoke(formatted_prompt.to_messages()).content.strip().lower()\n",
        "\n",
        "        if \"yes\" in decision:\n",
        "            return {\"next\": \"reflect\"}\n",
        "        else:\n",
        "            return {\"next\": \"finalize\"}\n",
        "    else:\n",
        "        return {\"next\": \"finalize\"}\n",
        "\n",
        "def finalize_response(state: GraphState) -> Dict:\n",
        "    \"\"\"Finalize the response after reflection iterations.\"\"\"\n",
        "    # Get the current query, final draft, and all reflections\n",
        "    query = state[\"messages\"][0].content\n",
        "    final_draft = state[\"response_draft\"]\n",
        "    all_reflections = \"\\n\\n\".join([f\"Reflection {i+1}: {reflection}\" for i, reflection in enumerate(state[\"reflections\"])])\n",
        "\n",
        "    # Create final response prompt\n",
        "    final_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=\"Create a final polished response that incorporates all reflection feedback.\"),\n",
        "        HumanMessage(content=f\"Query: {query}\\n\\nCurrent draft:\\n{final_draft}\\n\\nAll reflections:\\n{all_reflections}\\n\\nPlease provide the final polished response:\")\n",
        "    ])\n",
        "\n",
        "    # Generate final response\n",
        "    final_response = response_llm.invoke(final_prompt.format_prompt(input_variables={'query': query, 'final_draft': final_draft, 'all_reflections': all_reflections}).to_messages()) # Format the prompt and convert to BaseMessages\n",
        "\n",
        "    # Update state with final response and add to messages\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=final_response.content)],\n",
        "        \"final_response\": final_response.content,\n",
        "        \"next\": END\n",
        "    }\n",
        "\n",
        "# Build the graph\n",
        "def build_reflexion_graph():\n",
        "    graph = StateGraph(GraphState)\n",
        "\n",
        "    # Add nodes\n",
        "    graph.add_node(\"generate_initial_response\", generate_initial_response)\n",
        "    graph.add_node(\"reflect\", reflect_on_response)\n",
        "    graph.add_node(\"revise\", revise_response)\n",
        "    graph.add_node(\"decide\", decide_next_step)\n",
        "    graph.add_node(\"finalize\", finalize_response)\n",
        "\n",
        "    # Add edges\n",
        "    graph.add_edge(\"generate_initial_response\", \"reflect\")\n",
        "    graph.add_edge(\"reflect\", \"revise\")\n",
        "    graph.add_edge(\"revise\", \"decide\")\n",
        "    graph.add_conditional_edges(\n",
        "        \"decide\",\n",
        "        lambda x: x[\"next\"],\n",
        "        {\n",
        "            \"reflect\": \"reflect\",\n",
        "            \"finalize\": \"finalize\"\n",
        "        }\n",
        "    )\n",
        "    graph.add_edge(\"finalize\", END)\n",
        "\n",
        "    # Set entry point\n",
        "    graph.set_entry_point(\"generate_initial_response\")\n",
        "\n",
        "    return graph.compile()\n",
        "\n",
        "# Create a runnable graph\n",
        "reflexion_graph = build_reflexion_graph()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize state\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"What are the ethical implications of artificial general intelligence?\")],\n",
        "        \"reflections\": [],\n",
        "        \"response_draft\": \"\",\n",
        "        \"final_response\": \"\",\n",
        "        \"reflection_count\": 0,\n",
        "        \"max_reflections\": 3\n",
        "    }\n",
        "\n",
        "    # Run the graph with tracing\n",
        "    result = reflexion_graph.invoke(initial_state)\n",
        "\n",
        "    # Print final response\n",
        "    print(\"\\nFinal Response after Reflection:\")\n",
        "    print(result[\"final_response\"])\n",
        "\n",
        "    # Print reflection process\n",
        "    print(\"\\nReflection Process:\")\n",
        "    for i, reflection in enumerate(result[\"reflections\"]):\n",
        "        print(f\"\\nReflection {i+1}:\")\n",
        "        print(reflection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kBqE49eH3Da",
        "outputId": "2da706b2-d97f-4c07-ffe1-c2acbf587759"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response after Reflection:\n",
            "The ethical implications of artificial general intelligence (AGI) are profound and multifaceted, encompassing considerations related to autonomy, control, safety, societal impact, and more. As AGI remains a speculative concept, it is crucial to explore these implications with both caution and optimism, while clearly distinguishing between current AI capabilities and the hypothetical nature of AGI.\n",
            "\n",
            "1. **Autonomy and Control**: A major ethical concern is determining the level of autonomy AGI should possess. AGI systems could potentially make independent decisions, so ensuring their alignment with human values is crucial. This raises questions about maintaining human oversight and preventing AGI from acting against human interests. Developing ethical guidelines and control mechanisms, such as continuous \"alignment checks\"—which involve ensuring that AGI systems' goals and actions remain consistent with human values—is essential.\n",
            "\n",
            "2. **Safety and Risk**: AGI presents significant risks, including the potential to surpass human intelligence and behave unpredictably. To mitigate these risks, robust safety frameworks and protocols are necessary. These should include fail-safes, which are systems designed to revert AGI to a safe state in case of malfunction, and alignment checks to prevent harmful actions. Research into AI safety, by organizations like OpenAI and the Future of Humanity Institute, provides valuable insights into these challenges. Specific scenarios, such as AGI managing critical infrastructure, highlight the importance of these safety measures.\n",
            "\n",
            "3. **Societal Impact**: AGI has the potential to revolutionize industries, improve efficiency, and create new opportunities. For instance, it could lead to breakthroughs in healthcare by accelerating drug discovery and personalizing treatments, or in environmental management by optimizing resource use. However, it also poses risks of economic displacement and increased inequality. Policymakers must plan for workforce transitions and ensure equitable distribution of AGI's benefits through education, retraining programs, and social safety nets. Exploring how AGI could complement human roles rather than replace them is crucial. Additionally, AGI could create new job opportunities in fields like AI maintenance and oversight, potentially offsetting some displacement effects.\n",
            "\n",
            "4. **Moral Status**: Should AGI attain consciousness or sentience, ethical debates about its moral status will emerge, including considerations of rights and ethical treatment. However, the possibility of AGI achieving consciousness is highly speculative. Engaging with interdisciplinary research in philosophy, cognitive science, and AI ethics is crucial to navigate these debates. Current scientific consensus suggests that AGI consciousness is a distant and uncertain possibility, and discussions should be framed within this speculative context.\n",
            "\n",
            "5. **Privacy and Surveillance**: The use of AGI in surveillance could enhance security but also threatens privacy. Developing regulations that balance these concerns is critical to prevent misuse and protect individual freedoms. Implementing transparent data governance frameworks and ensuring accountability in AGI systems can help safeguard privacy rights.\n",
            "\n",
            "6. **Bias and Fairness**: AGI systems must be designed to avoid reinforcing biases present in training data. Ensuring fairness requires diverse datasets and transparency in decision-making processes to build trust and equity. Bias can manifest in AI systems when they replicate societal prejudices, leading to unfair treatment of certain groups. Initiatives like the AI Now Institute's research on bias and fairness provide valuable guidelines for developing equitable AI systems.\n",
            "\n",
            "7. **Cultural Diversity and Environmental Impact**: The ethical implications of AGI must consider cultural diversity, as different societies may perceive and regulate AGI differently. Understanding these cultural perspectives is crucial for developing globally applicable ethical guidelines. Additionally, the environmental impact of developing and maintaining AGI systems, such as energy consumption, should be addressed to ensure sustainable practices. Case studies on the carbon footprint of AI models can provide insights into minimizing environmental impact.\n",
            "\n",
            "8. **International Cooperation and Regulation**: Given the global nature of AGI's potential impact, international cooperation and regulation are vital. Establishing global standards and agreements can help manage the ethical implications of AGI and prevent exacerbation of existing power imbalances. Collaborative efforts, such as those by the United Nations and proposals for international treaties on AI governance, are essential. Existing initiatives, like the OECD's AI Principles, aim to foster international dialogue and consensus on AI ethics.\n",
            "\n",
            "9. **Military Applications and Geopolitical Tensions**: The potential use of AGI in military applications raises significant ethical concerns, including the risk of autonomous weapons and escalation of conflicts. AGI could exacerbate geopolitical tensions by shifting power dynamics and creating arms races. International treaties and agreements are necessary to regulate the military use of AGI and prevent destabilizing effects on global security.\n",
            "\n",
            "In summary, the ethical considerations surrounding AGI necessitate proactive engagement from technologists, policymakers, ethicists, and society at large. By addressing potential risks and ensuring AGI development aligns with human values, we can work towards a future where AGI contributes positively to societal well-being. It is important to remain realistic about the current capabilities of AI and distinguish them from the hypothetical future of AGI to prevent misconceptions.\n",
            "\n",
            "Reflection Process:\n",
            "\n",
            "Reflection 1:\n",
            "The revised response draft provides a comprehensive overview of the ethical implications of artificial general intelligence (AGI), but there are several areas where it could be improved:\n",
            "\n",
            "1. **Factual Errors or Inconsistencies**: The response does not contain factual errors, but it could benefit from more specific examples or references to existing research or case studies to support its claims.\n",
            "\n",
            "2. **Incomplete or Partial Responses**: \n",
            "   - The section on \"Societal Impact\" mentions the potential for economic displacement and inequality but does not sufficiently explore the positive outcomes AGI could bring, such as advancements in healthcare, education, and environmental management. Including these aspects would provide a more balanced view.\n",
            "   - The \"Moral Status\" section briefly mentions the potential for AGI to attain consciousness or sentience but does not delve into the complexities of defining and measuring these states. A deeper exploration of the philosophical and scientific challenges in this area would enhance the discussion.\n",
            "\n",
            "3. **Logical Fallacies or Weak Reasoning**: The response generally avoids logical fallacies, but it could strengthen its reasoning by providing more detailed arguments or counterarguments, particularly in areas where there is significant debate, such as the moral status of AGI.\n",
            "\n",
            "4. **Unclear or Ambiguous Language**: The language is mostly clear, but terms like \"alignment checks\" and \"fail-safes\" in the \"Safety and Risk\" section could be better defined or explained for readers who may not be familiar with these concepts.\n",
            "\n",
            "5. **Missing Important Considerations or Perspectives**: \n",
            "   - The response could benefit from discussing the role of international cooperation and regulation in managing AGI's ethical implications, as these technologies will likely have global impacts.\n",
            "   - It could also address the potential for AGI to exacerbate existing power imbalances, both within and between countries, and the ethical considerations that arise from this.\n",
            "\n",
            "6. **Potential Harmful or Misleading Content**: There is no harmful or misleading content, but the response should be careful not to overstate the current capabilities of AGI, as true AGI has not yet been realized. Clarifying the distinction between current AI technologies and the hypothetical future of AGI would prevent misconceptions.\n",
            "\n",
            "Overall, the response is well-structured and covers a broad range of ethical considerations, but it would benefit from more depth, specific examples, and a balanced exploration of both positive and negative implications.\n",
            "\n",
            "Reflection 2:\n",
            "The response draft provides a comprehensive overview of the ethical implications of artificial general intelligence (AGI), but there are several areas where it could be improved:\n",
            "\n",
            "1. **Factual Errors or Inconsistencies**: The response correctly identifies AGI as a theoretical concept, but it could benefit from a clearer distinction between current AI capabilities and the hypothetical nature of AGI. This would help prevent misconceptions about the current state of AI technology.\n",
            "\n",
            "2. **Incomplete or Partial Responses**: While the response covers a wide range of ethical considerations, it could delve deeper into specific examples or case studies to illustrate the points made. For instance, discussing specific scenarios where AGI might pose risks or benefits would provide a more concrete understanding.\n",
            "\n",
            "3. **Logical Fallacies or Weak Reasoning**: The response assumes that AGI will inevitably lead to economic displacement and increased inequality without considering potential counterarguments or solutions. It would be beneficial to explore how AGI could be integrated into the workforce in a way that minimizes these risks.\n",
            "\n",
            "4. **Unclear or Ambiguous Language**: The term \"alignment checks\" is used without a clear explanation of what it entails or how it would be implemented. Providing a more detailed description or example would enhance clarity.\n",
            "\n",
            "5. **Missing Important Considerations or Perspectives**: The response could benefit from a discussion on the ethical implications of AGI in terms of cultural diversity and how different societies might perceive and regulate AGI differently. Additionally, the potential environmental impact of developing and maintaining AGI systems is not addressed.\n",
            "\n",
            "6. **Potential Harmful or Misleading Content**: The response mentions the possibility of AGI attaining consciousness or sentience, which is a highly speculative and controversial topic. It would be prudent to clarify that this is a theoretical consideration and not an established fact, to avoid misleading readers.\n",
            "\n",
            "7. **International Cooperation and Regulation**: While the response mentions the importance of international cooperation, it could provide more specific examples of existing efforts or proposals for global governance of AGI, such as the role of specific international treaties or organizations.\n",
            "\n",
            "Overall, the response is well-structured and covers many key points, but it would benefit from more detailed explanations, examples, and consideration of alternative perspectives to provide a more balanced and informative discussion.\n",
            "\n",
            "Reflection 3:\n",
            "The response draft provides a comprehensive overview of the ethical implications of artificial general intelligence (AGI), but there are several areas where it could be improved:\n",
            "\n",
            "1. **Factual Errors or Inconsistencies**: The draft correctly notes that AGI is a theoretical concept, but it sometimes discusses AGI as if its development is imminent. This could mislead readers into overestimating the current state of AI technology. Clarifying the speculative nature of AGI and the current limitations of AI would provide a more accurate context.\n",
            "\n",
            "2. **Incomplete or Partial Responses**: While the draft covers a wide range of ethical considerations, it could delve deeper into some areas. For example, the section on \"Bias and Fairness\" could benefit from more specific examples of how biases manifest in AI systems and the challenges in mitigating them. Additionally, the discussion on \"Moral Status\" could explore more scenarios and implications if AGI were to achieve consciousness.\n",
            "\n",
            "3. **Logical Fallacies or Weak Reasoning**: The draft assumes that AGI will inevitably lead to economic displacement and increased inequality without considering potential counterarguments or solutions. It would be beneficial to explore how AGI might create new job opportunities or how economies might adapt to technological advancements.\n",
            "\n",
            "4. **Unclear or Ambiguous Language**: Terms like \"alignment checks\" and \"fail-safes\" are introduced without sufficient explanation. Providing clearer definitions or examples would help readers understand these concepts better.\n",
            "\n",
            "5. **Missing Important Considerations or Perspectives**: The draft could benefit from a discussion on the ethical implications of AGI in military applications, which is a significant area of concern. Additionally, the potential for AGI to exacerbate existing geopolitical tensions is not addressed.\n",
            "\n",
            "6. **Potential Harmful or Misleading Content**: The draft mentions the possibility of AGI achieving consciousness, which is highly speculative and could lead to misconceptions. It would be prudent to emphasize the current scientific consensus on this topic and the speculative nature of such claims.\n",
            "\n",
            "7. **Cultural Diversity and Environmental Impact**: While these are important considerations, the draft could provide more concrete examples or case studies to illustrate how cultural diversity and environmental impact might influence AGI development and deployment.\n",
            "\n",
            "8. **International Cooperation and Regulation**: The draft mentions international cooperation but could provide more detail on existing efforts and challenges in achieving global consensus on AGI regulation. Highlighting specific initiatives or agreements would strengthen this section.\n",
            "\n",
            "Overall, the response draft is a solid foundation but would benefit from more detailed exploration of certain topics, clearer explanations of technical terms, and a balanced discussion of potential benefits and challenges associated with AGI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. The Chain-of-Thought Pattern"
      ],
      "metadata": {
        "id": "-eWs6dWnH3SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, TypedDict, Annotated, Sequence, Tuple, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, FunctionMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ABC\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"XYZ\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"cot-pattern-demo\"\n",
        "\n",
        "# Create a state for our graph\n",
        "class GraphState(TypedDict):\n",
        "    messages: Annotated[Sequence[Any], \"The messages in the conversation so far\"]\n",
        "    problem: Annotated[str, \"The problem to be solved\"]\n",
        "    reasoning_steps: Annotated[List[str], \"The chain of reasoning steps\"]\n",
        "    current_step: Annotated[int, \"The current reasoning step number\"]\n",
        "    final_answer: Annotated[str, \"The final answer to the problem\"]\n",
        "\n",
        "# Create system messages\n",
        "PROBLEM_ANALYZER_SYSTEM = \"\"\"You are an expert problem analyzer that breaks down complex problems into simpler steps.\n",
        "When given a problem, analyze it and create a step-by-step plan to solve it.\n",
        "Focus on identifying the key components and the logical sequence of steps needed to reach the solution.\n",
        "\"\"\"\n",
        "\n",
        "REASONING_SYSTEM = \"\"\"You are an expert problem solver that uses careful reasoning to solve problems.\n",
        "You will be given a specific step in a reasoning chain to complete.\n",
        "Focus only on this specific step. Do not attempt to solve the entire problem.\n",
        "Show your work clearly and explain your reasoning for this step.\n",
        "\"\"\"\n",
        "\n",
        "ANSWER_SYSTEM = \"\"\"You are a solution summarizer that creates clear and concise final answers.\n",
        "Based on the complete chain of reasoning steps provided, synthesize a final answer to the original problem.\n",
        "Be precise and ensure your answer directly addresses the original question.\n",
        "\"\"\"\n",
        "\n",
        "# Define LLM instances\n",
        "analyzer_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "reasoning_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "answer_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Define node functions\n",
        "def analyze_problem(state: GraphState) -> Dict:\n",
        "    \"\"\"Analyze the problem and create a plan of reasoning steps.\"\"\"\n",
        "    # Extract problem from messages\n",
        "    problem = state[\"messages\"][0].content\n",
        "\n",
        "    # Create analyzer prompt\n",
        "    analyzer_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=PROBLEM_ANALYZER_SYSTEM),\n",
        "        HumanMessage(content=f\"Analyze this problem and break it down into a step-by-step reasoning plan. Each step should build logically on the previous steps:\\n\\n{problem}\")\n",
        "    ])\n",
        "\n",
        "    # Generate analysis\n",
        "    analysis = analyzer_llm.invoke(analyzer_prompt.format_prompt(input=problem).to_messages()) # Format the prompt and convert to BaseMessages\n",
        "\n",
        "    # Extract reasoning steps from the analysis\n",
        "    steps_text = analysis.content\n",
        "    steps = re.findall(r'(?:Step \\d+:|^\\d+\\.)(.*?)(?=(?:Step \\d+:|^\\d+\\.)|$)', steps_text, re.DOTALL | re.MULTILINE)\n",
        "\n",
        "    if not steps:  # Fallback if regex doesn't match\n",
        "        steps = steps_text.split('\\n\\n')\n",
        "\n",
        "    # Clean up the steps\n",
        "    steps = [step.strip() for step in steps if step.strip()]\n",
        "\n",
        "    # Update state\n",
        "    return {\n",
        "        \"problem\": problem,\n",
        "        \"reasoning_steps\": steps,\n",
        "        \"current_step\": 0\n",
        "    }\n",
        "\n",
        "def execute_reasoning_step(state: GraphState) -> Dict:\n",
        "    \"\"\"Execute the current reasoning step.\"\"\"\n",
        "    # Get the problem and current step information\n",
        "    problem = state[\"problem\"]\n",
        "    steps = state[\"reasoning_steps\"]\n",
        "    current_step_idx = state[\"current_step\"]\n",
        "\n",
        "    # Check if we have steps to execute\n",
        "    if current_step_idx >= len(steps):\n",
        "        return {\"next\": \"generate_answer\"}\n",
        "\n",
        "    current_step = steps[current_step_idx]\n",
        "    previous_steps = steps[:current_step_idx]\n",
        "    previous_steps_text = \"\\n\".join([f\"Step {i+1}: {step}\" for i, step in enumerate(previous_steps)])\n",
        "\n",
        "    # Create reasoning prompt\n",
        "    reasoning_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=REASONING_SYSTEM),\n",
        "        HumanMessage(content=f\"\"\"\n",
        "Problem: {problem}\n",
        "\n",
        "Previous reasoning steps:\n",
        "{previous_steps_text}\n",
        "\n",
        "Current step to execute (Step {current_step_idx + 1}): {current_step}\n",
        "\n",
        "Please complete this specific reasoning step. Show your work and explain your reasoning clearly.\n",
        "\"\"\")\n",
        "    ])\n",
        "\n",
        "    # Generate reasoning for this step\n",
        "    reasoning = reasoning_llm.invoke(reasoning_prompt.format_prompt().to_messages()) # Format the prompt and convert it to BaseMessages\n",
        "\n",
        "    # Update the step with the detailed reasoning\n",
        "    updated_steps = state[\"reasoning_steps\"].copy()\n",
        "    updated_steps[current_step_idx] = f\"{current_step}\\n\\nReasoning: {reasoning.content}\"\n",
        "\n",
        "    # Update state\n",
        "    return {\n",
        "        \"reasoning_steps\": updated_steps,\n",
        "        \"current_step\": current_step_idx + 1\n",
        "    }\n",
        "\n",
        "def decide_next_action(state: GraphState) -> Dict:\n",
        "    \"\"\"Decide whether to continue reasoning or generate final answer.\"\"\"\n",
        "    current_step = state[\"current_step\"]\n",
        "    total_steps = len(state[\"reasoning_steps\"])\n",
        "\n",
        "    if current_step < total_steps:\n",
        "        return {\"next\": \"reasoning\"}\n",
        "    else:\n",
        "        return {\"next\": \"generate_answer\"}\n",
        "\n",
        "def generate_answer(state: GraphState) -> Dict:\n",
        "    \"\"\"Generate the final answer based on the complete reasoning chain.\"\"\"\n",
        "    # Get the problem and reasoning steps\n",
        "    problem = state[\"problem\"]\n",
        "    steps = state[\"reasoning_steps\"]\n",
        "    steps_text = \"\\n\\n\".join([f\"Step {i+1}: {step}\" for i, step in enumerate(steps)])\n",
        "\n",
        "    # Create answer prompt\n",
        "    answer_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=ANSWER_SYSTEM),\n",
        "        HumanMessage(content=f\"\"\"\n",
        "Original problem: {problem}\n",
        "\n",
        "Complete chain of reasoning:\n",
        "{steps_text}\n",
        "\n",
        "Based on this chain of reasoning, what is the final answer to the original problem?\n",
        "Provide a clear and concise summary of the solution.\n",
        "\"\"\")\n",
        "    ])\n",
        "\n",
        "    # Generate final answer\n",
        "    answer = answer_llm.invoke(answer_prompt.format_prompt(input_variables={'problem': problem, 'steps_text': steps_text}).to_messages())\n",
        "\n",
        "    # Update state with final answer and add to messages\n",
        "    response_message = f\"\"\"\n",
        "Here's my solution to the problem:\n",
        "\n",
        "{answer.content}\n",
        "\n",
        "My reasoning process:\n",
        "{steps_text}\n",
        "\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=response_message)],\n",
        "        \"final_answer\": answer.content,\n",
        "        \"next\": END\n",
        "    }\n",
        "\n",
        "# Build the graph\n",
        "def build_cot_graph():\n",
        "    graph = StateGraph(GraphState)\n",
        "\n",
        "    # Add nodes\n",
        "    graph.add_node(\"analyze_problem\", analyze_problem)\n",
        "    graph.add_node(\"reasoning\", execute_reasoning_step)\n",
        "    graph.add_node(\"decision\", decide_next_action)\n",
        "    graph.add_node(\"generate_answer\", generate_answer)\n",
        "\n",
        "    # Add edges\n",
        "    graph.add_edge(\"analyze_problem\", \"reasoning\")\n",
        "    graph.add_edge(\"reasoning\", \"decision\")\n",
        "    graph.add_conditional_edges(\n",
        "        \"decision\",\n",
        "        lambda x: x[\"next\"],\n",
        "        {\n",
        "            \"reasoning\": \"reasoning\",\n",
        "            \"generate_answer\": \"generate_answer\"\n",
        "        }\n",
        "    )\n",
        "    graph.add_edge(\"generate_answer\", END)\n",
        "\n",
        "    # Set entry point\n",
        "    graph.set_entry_point(\"analyze_problem\")\n",
        "\n",
        "    return graph.compile()  # Removed the checkpointer (MemoryCheckpoint)\n",
        "\n",
        "# Create a runnable graph\n",
        "cot_graph = build_cot_graph()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize state with a complex math problem\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"A boat travels upstream for 2 hours at 10 km/h and then returns downstream covering the same distance in 1 hour. What is the speed of the current?\")],\n",
        "        \"problem\": \"\",\n",
        "        \"reasoning_steps\": [],\n",
        "        \"current_step\": 0,\n",
        "        \"final_answer\": \"\"\n",
        "    }\n",
        "\n",
        "    # Run the graph with tracing\n",
        "    result = cot_graph.invoke(initial_state)\n",
        "\n",
        "    # Print final answer\n",
        "    print(\"\\nFinal Answer:\")\n",
        "    print(result[\"final_answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4lik3FiMKpq",
        "outputId": "c13b29f8-9dd3-4002-f689-5bb10d0748f1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Answer:\n",
            "The speed of the current is approximately 3.33 km/h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The RAG (Retrieval-Augmented Generation) Pattern"
      ],
      "metadata": {
        "id": "HVudPvcOMK5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tktbOt8zgF-o",
        "outputId": "a8d63343-982b-416f-b424-faeb03ebe8e6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abbEkWg0gMAK",
        "outputId": "58036f1a-117d-4e67-b78f-fd60fe8f608a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.30)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.24.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.32.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.53b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.53b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.32.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.32.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.53b0-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.32.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.24.1-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=14eeaa8fdcff46387ab7b0b88a1ef5e3f2a69b16aae9fbd20de3f95b276e57a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.31.1\n",
            "    Uninstalling opentelemetry-api-1.31.1:\n",
            "      Successfully uninstalled opentelemetry-api-1.31.1\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.31.1\n",
            "    Uninstalling opentelemetry-sdk-1.31.1:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.31.1\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.4 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-api-1.32.0 opentelemetry-exporter-otlp-proto-common-1.32.0 opentelemetry-exporter-otlp-proto-grpc-1.32.0 opentelemetry-instrumentation-0.53b0 opentelemetry-instrumentation-asgi-0.53b0 opentelemetry-instrumentation-fastapi-0.53b0 opentelemetry-proto-1.32.0 opentelemetry-sdk-1.32.0 opentelemetry-semantic-conventions-0.53b0 opentelemetry-util-http-0.53b0 overrides-7.7.0 posthog-3.24.1 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.45.3 uvicorn-0.34.1 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Pattern Implementation with LangGraph\n",
        "import os\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Define the state structure for our RAG flow\n",
        "class RAGState(dict):\n",
        "    \"\"\"State for the RAG application.\"\"\"\n",
        "    query: str\n",
        "    retrieval_results: List[Document] = None\n",
        "    context: str = None\n",
        "    answer: str = None\n",
        "\n",
        "# Step 1: Set up the document ingestion process\n",
        "def ingest_documents(docs_path: str, collection_name: str) -> Chroma:\n",
        "    \"\"\"Process documents and load them into a vector database.\"\"\"\n",
        "    # Simple document loading for demo purposes\n",
        "    documents = []\n",
        "    for filename in os.listdir(docs_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(docs_path, filename), 'r') as f:\n",
        "                text = f.read()\n",
        "                documents.append(Document(page_content=text, metadata={\"source\": filename}))\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Create vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=\"./chroma_db\"\n",
        "    )\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# Step 2: Define the retrieval component\n",
        "def retrieve(state: RAGState) -> RAGState:\n",
        "    \"\"\"Retrieve relevant documents based on the query.\"\"\"\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=\"./chroma_db\",\n",
        "        embedding_function=OpenAIEmbeddings()\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    state[\"retrieval_results\"] = retriever.invoke(state[\"query\"])\n",
        "    return state\n",
        "\n",
        "# Step 3: Format the retrieved documents into context\n",
        "def format_context(state: RAGState) -> RAGState:\n",
        "    \"\"\"Format retrieved documents into a single context string.\"\"\"\n",
        "    if state[\"retrieval_results\"]:\n",
        "        context_texts = [doc.page_content for doc in state[\"retrieval_results\"]]\n",
        "        sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in state[\"retrieval_results\"]]\n",
        "\n",
        "        formatted_context = \"\\n\\n\".join([\n",
        "            f\"Source: {sources[i]}\\nContent: {context_texts[i]}\"\n",
        "            for i in range(len(context_texts))\n",
        "        ])\n",
        "\n",
        "        state[\"context\"] = formatted_context\n",
        "    else:\n",
        "        state[\"context\"] = \"No relevant information found.\"\n",
        "\n",
        "    return state\n",
        "\n",
        "# Step 4: Generate an answer using the LLM\n",
        "def generate_answer(state: RAGState) -> RAGState:\n",
        "    \"\"\"Generate an answer using the retrieved context.\"\"\"\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "    You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
        "    If the context doesn't contain relevant information, say so and try to provide general information.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "    # Create the chain\n",
        "    chain = (\n",
        "        prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Generate the answer\n",
        "    state[\"answer\"] = chain.invoke({\n",
        "        \"context\": state[\"context\"],\n",
        "        \"query\": state[\"query\"]\n",
        "    })\n",
        "\n",
        "    return state\n",
        "\n",
        "# Step 5: Build the LangGraph\n",
        "def build_rag_graph() -> StateGraph:\n",
        "    \"\"\"Build the RAG workflow graph.\"\"\"\n",
        "    workflow = StateGraph(RAGState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"retrieve\", retrieve)\n",
        "    workflow.add_node(\"format_context\", format_context)\n",
        "    workflow.add_node(\"generate_answer\", generate_answer)\n",
        "\n",
        "    # Add edges\n",
        "    workflow.add_edge(\"retrieve\", \"format_context\")\n",
        "    workflow.add_edge(\"format_context\", \"generate_answer\")\n",
        "    workflow.add_edge(\"generate_answer\", END)\n",
        "\n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Set up your document collection (do this once)\n",
        "    # ingest_documents(\"./documents\", \"my_knowledge_base\")\n",
        "\n",
        "    # Create the graph\n",
        "    rag_graph = build_rag_graph()\n",
        "\n",
        "    # Run the graph with a query\n",
        "    result = rag_graph.invoke({\"query\": \"What is the capital of France?\"})\n",
        "\n",
        "    print(f\"Query: {result['query']}\")\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1WultICN4PS",
        "outputId": "d199c3fe-dfb1-494a-f484-aed00f118e67"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the capital of France?\n",
            "Answer: The context does not provide relevant information, but I can help with general knowledge. The capital of France is Paris.\n"
          ]
        }
      ]
    }
  ]
}